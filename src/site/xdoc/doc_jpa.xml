<?xml version="1.0" encoding="UTF-8"?>
<document xmlns="http://maven.apache.org/XDOC/2.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
			 xsi:schemaLocation="http://maven.apache.org/XDOC/2.0 http://maven.apache.org/xsd/xdoc-2.0.xsd">

	<properties>
		<title>JPA Server</title>
		<author email="jamesagnew@users.sourceforge.net">James Agnew</author>
	</properties>

	<body>

		<section name="JPA Server">

			<p>
				The HAPI FHIR
				<a href="./doc_rest_server.html">RestfulServer</a>
				module can be used to create a FHIR server endpoint against an arbitrary
				data source, which could be a database of your own design, an existing
				clinical system, a set of files, or anything else you come up with.
			</p>
			<p>
				HAPI also provides a persistence module which can be used to
				provide a complete RESTful server implementation, backed by a database of
				your choosing. This module uses the
				<a href="http://en.wikipedia.org/wiki/Java_Persistence_API">JPA 2.0</a>
				API to store data in a database without depending on any specific database technology.
			</p>
			<p>
				<b>Important Note:</b>
				This implementation uses a fairly simple table design, with a
				single table being used to hold resource bodies (which are stored as
				CLOBs, optionally GZipped to save space) and a set of tables to hold search indexes, tags,
				history details, etc. This design is only one of many possible ways
				of designing a FHIR server so it is worth considering whether it
				is appropriate for the problem you are trying to solve.
			</p>

			<subsection name="Getting Started">

				<p>
					The easiest way to get started with HAPI's JPA server module is
					to begin with the example project. There is a complete sample project
					found in our GitHub repo here:
					<a href="https://github.com/jamesagnew/hapi-fhir/tree/master/hapi-fhir-jpaserver-example">
						hapi-fhir-jpaserver-example
					</a>
				</p>

				<p>
					This example is a fully contained FHIR server, supporting all standard operations
					(read/create/delete/etc).
					It bundles an embedded instance of the <a href="http://db.apache.org/derby/">Apache Derby</a> Java
					database
					so that the server can run without depending on any external database, but it can also be
					configured to use an installation of Oracle, Postgres, etc.
				</p>

				<p>
					To take this project for a spin, check out the sources from GitHib (or download a snapshot),
					and then build the project:
				</p>

				<source><![CDATA[$ cd hapi-fhir-jpaserver-example
$ mvn install]]></source>

				<p>
					You now have two options for starting the server:
				</p>
				<ul>
					<li>
						<b>Deploy to Tomcat/JBoss/Websphere/etc:</b>
						You will now have a file
						in your <code>target</code> directory called <code>hapi-fhir-jpaserver-example.war</code>.
						This WAR file can be deployed to any Servlet container, at which point you could
						access the server by pointing your browser at a URL similar to the following
						(you may need to adjust the
						port depending on which port your container is configured to listen on):
						<a href="http://localhost:8080/hapi-fhir-jpaserver-example/">
							http://localhost:8080/hapi-fhir-jpaserver-example/
						</a>
					</li>
					<li>
						<b>Run with Maven and Embedded Jetty:</b>
						To start the server
						directly within Maven, you can execute the following command:
						<br/>
						<source>$ mvn jetty:run</source>
						You can then access the server by pointing your browser at the following URL:
						<a href="http://localhost:8080/hapi-fhir-jpaserver-example/">
							http://localhost:8080/hapi-fhir-jpaserver-example/
						</a>
					</li>
				</ul>
			</subsection>
		</section>

		<section name="Configuring The JPA Server">

			<p>
				The JPA server is configured through a series of configuration files, most
				of which are documented inline.
			</p>
			<ul>
				<li>
					<a href="https://github.com/jamesagnew/hapi-fhir/blob/master/hapi-fhir-jpaserver-example/src/main/java/ca/uhn/fhir/jpa/demo/FhirServerConfig.java">
						<b>FhirServerConfig.java</b>
					</a>
					:
					Configures the database connection settings
				</li>
			</ul>

		</section>

		<section name="DaoConfig">

			<p>
				The Spring confguration contains a definition for a bean called <code>daoConfig</code>,
				which will look something like the following:
			</p>
			<source><![CDATA[@Bean
public DaoConfig daoConfig() {
	DaoConfig retVal = new DaoConfig();
	retVal.setAllowMultipleDelete(true);
	retVal.setAllowInlineMatchUrlReferences(true);
	return retVal;
}]]></source>

			<p>
				You can use this method to change various configuration settings on the DaoConfig bean
				which define the way that the JPA server will behave.
				See the
				<a href="./apidocs-jpaserver/ca/uhn/fhir/jpa/dao/DaoConfig.html">DaoConfig JavaDoc</a>
				for information about the available settings.
			</p>

			<subsection name="External/Absolute Resource References">

				<p>
					Clients may sometimes post resources to your server that contain
					absolute resource references. For example, consider the following resource:
				</p>
				<source><![CDATA[<Patient xmlns="http://hl7.org/fhir">
   <id value="patient-infant-01"/>
   <name>
      <use value="official"/>
      <family value="Miller"/>
      <given value="Samuel"/>
   </name>
   <managingOrganization>
      <reference value="http://example.com/fhir/Organization/123"/>
   </managingOrganization>
</Patient>]]></source>

				<p>
					By default, the server will reject this reference, as only
					local references are permitted by the server. This can be changed
					however.
				</p>
				<p>
					If you want the server to recognize that this URL is actually a local
					reference (i.e. because the server will be deployed to the base URL
					<code>http://example.com/fhir/</code>) you can
					configure the server to recognize this URL via the following DaoConfig
					setting:
				</p>
				<source><![CDATA[@Bean
public DaoConfig daoConfig() {
	DaoConfig retVal = new DaoConfig();
	// ... other config ...
	retVal.getTreatBaseUrlsAsLocal().add("http://example.com/fhir/");
	return retVal;
}]]></source>

				<p>
					On the other hand, if you want the server to be configurable to
					allow remote references, you can set this with the confguration below.
					Using the <code>setAllowExternalReferences</code> means that
					it will be possible to search for references that refer to these
					external references.
				</p>

				<source><![CDATA[@Bean
public DaoConfig daoConfig() {
	DaoConfig retVal = new DaoConfig();
	// Allow external references
	retVal.setAllowExternalReferences(true);
	
	// If you are allowing external references, it is recommended to
	// also tell the server which references actually will be local
	retVal.getTreatBaseUrlsAsLocal().add("http://mydomain.com/fhir");
	return retVal;
}]]></source>
			</subsection>

			<subsection name="Logical References">

				<p>
					In some cases, you may have references which are <i>Logical References</i>,
					which means that they act as an identifier and not necessarily as a literal
					web address.
				</p>
				<p>
					A common use for logical references is in references to conformance
					resources, such as ValueSets, StructureDefinitions, etc. For example,
					you might refer to the ValueSet
					<code>http://hl7.org/fhir/ValueSet/quantity-comparator</code>
					from your own resources. In this case, you are not neccesarily telling
					the server that this is a real address that it should resolve, but
					rather that this is an identifier for a ValueSet where
					<code>ValueSet.url</code>
					has the given URI/URL.
				</p>
				<p>
					HAPI can be configured to treat certain URI/URL patterns as
					logical by using the DaoConfig#setTreatReferencesAsLogical property
					(see <a
					href="./apidocs-jpaserver/ca/uhn/fhir/jpa/dao/DaoConfig.html#setTreatReferencesAsLogical-java.util.Set-">
					JavaDoc</a>).
					For example:
				</p>
				<div class="source">
					<pre>
						// Treat specific URL as logical
						myDaoConfig.getTreatReferencesAsLogical().add("http://mysystem.com/ValueSet/cats-and-dogs");

						// Treat all references with given prefix as logical
						myDaoConfig.getTreatReferencesAsLogical().add("http://mysystem.com/mysystem-vs-*");
					</pre>
				</div>
				<a name="search_result caching"/>
			</subsection>

			<subsection name="Search Result Caching">

				<p>
					By default, search results will be cached for one minute. This means that
					if a client performs a search for <code>Patient?name=smith</code> and gets back
					500 results, if a client performs the same search within 60000 milliseconds the
					previously loaded search results will be returned again. This also means that
					any new Patient resources named "Smith" within the last minute will not be
					reflected in the results.
				</p>
				<p>
					Under many normal scenarios this is a n acceptable performance tradeoff,
					but in some cases it is not. If you want to disable caching, you have two
					options:
				</p>
				<p>
					<b>Globally Disable / Change Caching Timeout</b>
				</p>
				<p>
					You can change the global cache using the following setting:
				</p>
				<div class="source">
					<pre>
						myDaoConfig.setReuseCachedSearchResultsForMillis(null);
					</pre>
				</div>
				<p>
					<b>Disable Cache at the Request Level</b>
				</p>
				<p>
					Clients can selectively disable caching for an individual request
					using the Cache-Control header:
				</p>
				<div class="source">
					<pre>
						Cache-Control: no-cache
					</pre>
				</div>
				<p>
					<b>Disable Paging at the Request Level</b>
				</p>
				<p>
					If the client knows that they will only want a small number of results
					(for example, a UI containing 20 results is being shown and the client
					knows that they will never load the next page of results) the client
					may also use the <code>no-store</code> directive along with a HAPI FHIR
					extension called <code>max-results</code> in order to specify that
					only the given number of results should be fetched. This directive
					disabled paging entirely for the request and causes the request to
					return immediately when the given number of results is found. This
					can cause a noticeable performance improvement in some cases.
				</p>
				<div class="source">
					<pre>
						Cache-Control: no-store, max-results=20
					</pre>
				</div>

			</subsection>

		</section>

		<section name="Architecture">

			<img src="images/jpa_architecture.png" alt="Architecture" align="right"/>

			<p>
				The HAPI JPA Server has the following components:
			</p>

			<ul>
				<li>
					<b>Resource Providers:</b>
					A RESTful server <a href="./doc_rest_server.html#resource_providers">Resource Provider</a> is
					provided for each resource type in a given release of FHIR. Each resource provider implements
					a
					<a href="./apidocs/ca/uhn/fhir/rest/annotation/Search.html">@Search</a>
					method implementing the complete set of search parameters defined in the FHIR
					specification for the given resource type.
					<br/>
					<br/>
					The resource providers also extend a superclass which implements all of the
					other FHIR methods, such as Read, Create, Delete, etc.
					<br/>
					<br/>
					Note that these resource providers are generated as a part of the HAPI build process,
					so they are not checked into Git. You can see their source
					in the <a href="./xref-jpaserver/">JXR Report</a>,
					for example the
					<a href="./xref-jpaserver/ca/uhn/fhir/jpa/rp/dstu2/PatientResourceProvider.html">
						PatientResourceProvider</a>.
					<br/>
					<br/>
					The resource providers do not actually implement any of the logic
					in searching, updating, etc. They simply receive the incoming HTTP calls (via the RestfulServer)
					and pass along the incoming requests to the DAOs.
					<br/>
					<br/>
				</li>
				<li>
					<b>HAPI DAOs:</b>
					The DAOs actually implement all of the database business logic relating to
					the storage, indexing, and retrieval of FHIR resources, using the underlying JPA
					API.
					<br/>
					<br/>
				</li>
				<li>
					<b>Hibernate:</b>
					The HAPI JPA Server uses the JPA library, implemented by Hibernate. No Hibernate
					specific features are used, so the library should also work with other
					providers (e.g. Eclipselink) but it is not tested regularly with them.
					<br/>
					<br/>
				</li>
				<li>
					<b>Database:</b>
					The RESTful server uses an embedded Derby database, but can be configured to
					talk to
					<a href="https://developer.jboss.org/wiki/SupportedDatabases2?_sscc=t">any database supported by
						Hibernate</a>.
				</li>

			</ul>

		</section>

		<section name="Additional Information">

			<ul>
				<li>
					<a href="https://www.openhealthhub.org/t/hapi-terminology-server-uk-snomed-ct-import/592">This page</a>
					has information on loading national editions (UK specifically) of SNOMED CT files into
					the database.
				</li>
			</ul>

		</section>

		<!-- 
		alter table hfj_res_link ALTER COLUMN "TARGET_RESOURCE_ID" NULL;
		
		select sp_index_status, count(*) from hfj_resource group by sp_index_status
delete from hfj_history_tag where res_id in (select res_id from hfj_resource where sp_index_status = 2);
delete from hfj_res_tag where res_id in (select res_id from hfj_resource where sp_index_status = 2);
delete from hfj_spidx_coords where res_id in (select res_id from hfj_resource where sp_index_status = 2);
delete from hfj_spidx_number where res_id in (select res_id from hfj_resource where sp_index_status = 2);
delete from hfj_spidx_quantity where res_id in (select res_id from hfj_resource where sp_index_status = 2);
delete from hfj_spidx_string where res_id in (select res_id from hfj_resource where sp_index_status = 2);
delete from hfj_spidx_token where res_id in (select res_id from hfj_resource where sp_index_status = 2);
delete from hfj_spidx_uri where res_id in (select res_id from hfj_resource where sp_index_status = 2);
delete from hfj_search_result where resource_pid in (select res_id from hfj_resource where sp_index_status = 2);
delete from hfj_res_link where src_resource_id in (select res_id from hfj_resource where sp_index_status = 2);
delete from hfj_res_link where target_resource_id in (select res_id from hfj_resource where sp_index_status = 2);
delete from hfj_subscription where res_id in (select res_id from hfj_resource where sp_index_status = 2);
delete from hfj_subscription_flag_res where res_id in (select res_id from hfj_resource where sp_index_status = 2);


delete from trm_concept_pc_link where pid in (select pid from trm_concept where codesystem_pid in (select pid from trm_codesystem_ver where res_id in (select res_id from hfj_resource where sp_index_status = 2)));
delete from trm_concept where codesystem_pid in (select pid from trm_codesystem_ver where res_id in (select res_id from hfj_resource where sp_index_status = 2));
delete from trm_codesystem_ver where res_id in (select res_id from hfj_resource where sp_index_status = 2);
delete from trm_codesystem where res_id in (select res_id from hfj_resource where sp_index_status = 2);

update hfj_resource set forced_id_pid = null where res_id in (select res_id from hfj_resource where sp_index_status = 2);
update hfj_res_ver set forced_id_pid = null where res_id in (select res_id from hfj_resource where sp_index_status = 2);
delete from hfj_forced_id where resource_pid in (select res_id from hfj_resource where sp_index_status = 2);
delete from hfj_resource where res_id in (select res_id from hfj_resource where sp_index_status = 2);
delete from hfj_res_ver where res_id in (select res_id from hfj_resource where sp_index_status = 2);
		
		
		
		 -->

		<a name="upgrading"/>
		<section name="Upgrading HAPI FHIR JPA">

			<p>
				HAPI FHIR JPA is a constantly evolving product, with new features being added to each
				new version of the library. As a result, it is generally necessary to execute a database
				migration as a part of an upgrade to HAPI FHIR.
			</p>

			<p>
				When upgrading the JPA server from one version of HAPI FHIR to a newer version,
				often there will be changes to the database schema. The
				<b>Migrate Database</b>
				command can be used to perform a migration from one version to the next.
			</p>

			<p>
				Note that this feature was added in HAPI FHIR 3.5.0. It is not able to migrate
				from versions prior to HAPI FHIR 3.4.0.
				<b>Please make a backup of your
					database before running this command!
				</b>
			</p>
			<p>
				The following example shows how to use the migrator utility to migrate between two versions.
			</p>
			<pre>./hapi-fhir-cli migrate-database -d DERBY_EMBEDDED -u
				"jdbc:derby:directory:target/jpaserver_derby_files;create=true" -n "" -p "" -f V3_4_0 -t V3_5_0
			</pre>

			<p>
				You may use the following command to get detailed help on the options:
			</p>
			<pre>./hapi-fhir-cli help migrate-database</pre>

			<p>
				Note the arguments:
				<ul>
					<li>
						<code>-d [dialect]</code>
						- This indicates the database dialect to use. See the detailed help for a list of options
					</li>
					<li>
						<code>-f [version]</code>
						- The version to migrate from
					</li>
					<li>
						<code>-t [version]</code>
						- The version to migrate to
					</li>
				</ul>
			</p>

			<subsection name="Oracle Support">
				<p>
					Note that the Oracle JDBC drivers are not distributed in the Maven Central repository,
					so they are not included in HAPI FHIR. In order to use this command with an Oracle database,
					you will need to invoke the CLI as follows:
				</p>
				<pre>java -cp hapi-fhir-cli.jar ca.uhn.fhir.cli.App migrate-database -d ORACLE_12C -u "[url]" -n
					"[username]" -p "[password]" -f V3_4_0 -t V3_5_0
				</pre>
			</subsection>

			<subsection name="Migrating 3.4.0 to 3.5.0+">
				<p>
					As of HAPI FHIR 3.5.0 a new mechanism for creating the JPA index tables (HFJ_SPIDX_xxx)
					has been implemented. This new mechanism uses hashes in place of large multi-column
					indexes. This improves both lookup times as well as required storage space. This change
					also paves the way for future ability to provide efficient multi-tenant searches (which
					is not yet implemented but is planned as an incremental improvement).
				</p>
				<p>
					This change is not a lightweight change however, as it requires a rebuild of the
					index tables in order to generate the hashes. This can take a long time on databases
					that already have a large amount of data.
				</p>
				<p>
					As a result, in HAPI FHIR JPA 3.6.0, an efficient way of upgrading existing databases
					was added. Under this new scheme, columns for the hashes are added but values are not
					calculated initially, database indexes are not modified on the HFJ_SPIDX_xxx tables,
					and the previous columns are still used for searching as was the case in HAPI FHIR
					JPA 3.4.0.
				</p>
				<p>
					In order to perform a migration using this functionality, the following steps should
					be followed:
				</p>
				<ul>
					<li>
						Stop your running HAPI FHIR JPA instance (and remember to make a backup of your
						database before proceeding with any changes!)
					</li>
					<li>
						Modify your <code>DaoConfig</code> to specify that hash-based searches should not be used, using
						the following setting:
						<br/>
						<pre>myDaoConfig.setDisableHashBasedSearches(true);</pre>
					</li>
					<li>
						Make sure that you have your JPA settings configured to not automatically
						create database indexes and columns using the following setting
						in your JPA Properties:
						<br/>
						<pre>extraProperties.put("hibernate.hbm2ddl.auto", "none");</pre>
					</li>
					<li>
						Run the database migrator command, including the entry
						<code>-x no-migrate-350-hashes</code>
						on the command line. For example:
						<br/>
						<pre>./hapi-fhir-cli migrate-database -d DERBY_EMBEDDED -u
							"jdbc:derby:directory:target/jpaserver_derby_files;create=true" -n "" -p "" -f V3_4_0 -t V3_6_0 -x
							no-migrate-350-hashes
						</pre>
					</li>
					<li>
						Rebuild and start your HAPI FHIR JPA server. At this point you should have a working
						HAPI FHIR JPA 3.6.0 server that is is still using HAPI FHIR 3.4.0 search indexes. Search hashes
						will be generated for any newly created or updated data but existing data will have null
						hashes.
					</li>
					<li>
						With the system running, request a complete reindex of the data in the database using
						an HTTP request such as the following:
						<br/>
						<pre>GET /$mark-all-resources-for-reindexing</pre>
						Note that this is a custom operation built into the HAPI FHIR JPA server. It should
						be secured in a real deployment, so Authentication is likely required for this
						call.
					</li>
					<li>
						You can track the reindexing process by watching your server logs,
						but also by using the following SQL executed directly against your database:
						<br/>
						<pre>SELECT * FROM HFJ_RES_REINDEX_JOB</pre>
						When this query no longer returns any rows, the reindexing process is complete.
					</li>
					<li>
						At this time, HAPI FHIR should be stopped once again in order to convert it
						to using the hash based indexes.
					</li>
					<li>
						Modify your <code>DaoConfig</code> to specify that hash-based searches are used, using
						the following setting (this is the default setting, so it could also simply
						be omitted):
						<br/>
						<pre>myDaoConfig.setDisableHashBasedSearches(false);</pre>
					</li>
					<li>
						Execute the migrator tool again, this time omitting the flag option, e.g.
						<br/>
						<pre>./hapi-fhir-cli migrate-database -d DERBY_EMBEDDED -u
							"jdbc:derby:directory:target/jpaserver_derby_files;create=true" -n "" -p "" -f V3_4_0 -t V3_6_0
						</pre>
					</li>
					<li>
						Rebuild, and start HAPI FHIR JPA again.
					</li>
				</ul>
			</subsection>

		</section>

		<section type="Cascading Deletes">
			<p>
				An interceptor called
				<code>CascadingDeleteInterceptor</code>
				may be registered against the Server. When this interceptor is enabled,
				cascading deletes may be performed using either of the following:
			</p>
			<ul>
				<li>The request may include the following parameter:
					<code>_cascade=delete</code>
				</li>
				<li>The request may include the following header:
					<code>X-Cascade: delete</code>
				</li>
			</ul>
		</section>

	</body>

</document>
